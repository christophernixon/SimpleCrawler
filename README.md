# Simple WebCrawler

### Considerations
- Testing: testing correct behaviour & performance
- Multi-threading
- Taking user input for domain
- Advantages of using statics
- Properly checking for successful codes
- Avoid anchors in urls
- It seems like the method for fetching all links on a page might be a bit broken?
- Handling robots.txt files
- DFS vs BFS?
- Throttling
- Ignoring PDF/Image urls


### Tradeoffs
- Simplification around finding pages on the same sub-domain, using `startsWith`. 

###Â Extensions
- What happens if needing to print to CSV?